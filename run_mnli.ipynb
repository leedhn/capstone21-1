{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "run_mnli.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leedhn/capstone21-1/blob/main/run_mnli.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjUirZiliFMa",
        "outputId": "9cb3d9cf-9ca1-4fc4-8201-f713fe5c0cb6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYJIOyBFSf9U"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX_TEq-mSw7o"
      },
      "source": [
        "#dir = '/content/drive/MyDrive/21-1_SoJong/20210407/'\n",
        "\n",
        "#PATH = '.inhouse_split_test_qids.txt' #train.jsonl 에서 가져온듯\n",
        "#CSQA_PATH ='train_rand_split.jsonl'\n",
        "#PRED_PATH ='.predictions_test.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNizwZIlUGcB",
        "outputId": "ef7ffb9d-3d0c-416a-a6de-395191b84da1"
      },
      "source": [
        "%cd '/content/drive/MyDrive/21-1_SoJong/20210407/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/21-1_SoJong/20210407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMSBZUfcUHvo",
        "outputId": "39447f4d-bbd0-413c-b8d9-9e8c10cb9b4e"
      },
      "source": [
        "!pip install fairseq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fairseq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/ab/92c6efb05ffdfe16fbdc9e463229d9af8c3b74dc943ed4b4857a87b223c2/fairseq-0.10.2-cp37-cp37m-manylinux1_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 12.3MB/s \n",
            "\u001b[?25hCollecting hydra-core\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e3/fbd70dd0d3ce4d1d75c22d56c0c9f895cfa7ed6587a9ffb821d6812d6a60/hydra_core-1.0.6-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 54.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.19.5)\n",
            "Collecting dataclasses\n",
            "  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq) (0.29.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq) (4.41.1)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.14.5)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq) (2019.12.20)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq) (1.8.1+cu101)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core->fairseq) (5.1.2)\n",
            "Collecting omegaconf<2.1,>=2.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq) (2.20)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core->fairseq) (3.4.1)\n",
            "Collecting PyYAML>=5.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 36.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=f070e7f356b8847143fab2c98d87af4c1d218b0c2d13fdd61cf1b814ce8d94a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: PyYAML, omegaconf, antlr4-python3-runtime, hydra-core, dataclasses, portalocker, sacrebleu, fairseq\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 antlr4-python3-runtime-4.8 dataclasses-0.6 fairseq-0.10.2 hydra-core-1.0.6 omegaconf-2.0.6 portalocker-2.0.0 sacrebleu-1.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgtofAuwqagB"
      },
      "source": [
        "#MNLI 돌리기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwbe3hYTqexu"
      },
      "source": [
        "MNLI 함수 def"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-LVLPd1U89W"
      },
      "source": [
        "import torch\n",
        "from fairseq.data.data_utils import collate_tokens\n",
        "\n",
        "class Contradiction:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.dir=''\n",
        "        self.txt=[]\n",
        "        self.batches=[] #Roberta에 넣을 batch 사이즈로 txt를 분할한 리스트\n",
        "        self.result_list=[] # result들 txt파일별로 묶기 위해 만듦\n",
        "        self.result=[] #Roberta prediction 결과 저장\n",
        "        self.contradiction_list=[] #txt파일별로 cotradiction이 나온 pair 저장\n",
        "        self.roberta = torch.hub.load('pytorch/fairseq', 'roberta.large.mnli') #사용할 모델\n",
        "        self.roberta.cuda() #gpu사용\n",
        "        self.roberta.eval()\n",
        "\n",
        "    #txt파일 batch사이즈로 분할해서 self.batches에 저장\n",
        "    def make_batch(self,size):\n",
        "        self.batches = [self.txt[i*size:(i+1)*size] for i in range((len(self.txt)+size-1)//size)]\n",
        "        #return self.batches\n",
        "\n",
        "    # roberta 돌려서 contradiction 모으기 (메인 기능)\n",
        "    def count_contradict(self):\n",
        "\n",
        "        self.result_list=[]\n",
        "        self.contradiction_list=[]\n",
        "        #0=contradiction\n",
        "        #1=natural\n",
        "        #2=entailment\n",
        "\n",
        "        k=0\n",
        "        for batch_of_pairs in self.batches:\n",
        "            \n",
        "            self.batch = collate_tokens(\n",
        "                [self.roberta.encode(pair[0], pair[1]) for pair in batch_of_pairs], pad_idx=1\n",
        "            )\n",
        "\n",
        "            logprobs = self.roberta.predict('mnli', self.batch)\n",
        "            self.result=logprobs.argmax(dim=1)\n",
        "            #print(k,'th batch')\n",
        "            #print(result)\n",
        "            self.result_list.extend(self.result)\n",
        "            for r in range(len(self.result)):\n",
        "                if self.result[r]==0:\n",
        "                    self.contradiction_list.append(batch_of_pairs[r])\n",
        "                    \n",
        "            k+=1\n",
        "           \n",
        "        #return (self.result_list, self.contradiction_list)\n",
        "\n",
        "    # 디렉토리에서 파일 읽어서 self.txt에 저장\n",
        "    def get_txt(self,dir): #txt 형식은 'ground truth' \\t 'predict' \\n ...\n",
        "        self.txt=[]\n",
        "        with open(dir,'r') as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines: \n",
        "                self.txt.append(line[:-1].split('\\t'))\n",
        "        #print(self.txt[:5][1])\n",
        "        #return self.txt\n",
        "\n",
        "    # 외부에서 클래스 함수 돌릴 때 사용. return값 있음\n",
        "    def run(self,dir,size):\n",
        "        self.get_txt(dir)\n",
        "        print('run file dir:',dir)\n",
        "        self.make_batch(size)\n",
        "        self.count_contradict()\n",
        "        len_contrad=len(self.contradiction_list)\n",
        "        len_txt=len(self.txt)\n",
        "        print(len_contrad)\n",
        "        #print(self.result_list)\n",
        "        print('file ',dir,'is done')\n",
        "        return (len_contrad,len_txt,(len_contrad/len_txt), self.contradiction_list, self.result_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayUibaFTqjio"
      },
      "source": [
        "MNLI 돌리기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5bxLh2gVC7j",
        "outputId": "1dc2dafe-8fe6-4445-e9b4-03f3e63b18ec"
      },
      "source": [
        "contrad_file={} #여기에 파일 별 결과들이 저장됩니다. key는 파일 이름. value는 결과값들.\n",
        "\n",
        "batch_size=16 #추후에 수정 가능\n",
        "\n",
        "#filename_list=['oracle_cont','masked100_cont']\n",
        "filename_list=['test', 'dev', 'train']\n",
        "\n",
        "file_type='txt'\n",
        "file_dir='/content/drive/MyDrive/21-1_SoJong/20210407/mnli_input'\n",
        "\n",
        "model=Contradiction()\n",
        "\n",
        "for name in filename_list:\n",
        "    dir= file_dir+'/'+name+'.'+file_type\n",
        "    print(dir)\n",
        "    contrad_file[name]=model.run(dir,batch_size)\n",
        "    # [contradiction 개수, txt 전체 개수, 비율, contradiction 전문, roberta prediction 리스트]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/fairseq/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "100%|██████████| 751652118/751652118 [01:06<00:00, 11254957.40B/s]\n",
            "1042301B [00:01, 1040114.29B/s]\n",
            "456318B [00:00, 620567.14B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/21-1_SoJong/20210407/mnli_input/test.txt\n",
            "run file dir: /content/drive/MyDrive/21-1_SoJong/20210407/mnli_input/test.txt\n",
            "6361\n",
            "file  /content/drive/MyDrive/21-1_SoJong/20210407/mnli_input/test.txt is done\n",
            "/content/drive/MyDrive/21-1_SoJong/20210407/mnli_input/dev.txt\n",
            "run file dir: /content/drive/MyDrive/21-1_SoJong/20210407/mnli_input/dev.txt\n",
            "7103\n",
            "file  /content/drive/MyDrive/21-1_SoJong/20210407/mnli_input/dev.txt is done\n",
            "/content/drive/MyDrive/21-1_SoJong/20210407/mnli_input/train.txt\n",
            "run file dir: /content/drive/MyDrive/21-1_SoJong/20210407/mnli_input/train.txt\n",
            "54883\n",
            "file  /content/drive/MyDrive/21-1_SoJong/20210407/mnli_input/train.txt is done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivoropfY5o-c",
        "outputId": "fab3d350-c545-4a74-d2b4-865ebf0f62ce"
      },
      "source": [
        "###### 수습 (mnli_input/txt 파일들 길이 반씩으로 자르기)\n",
        "# import os\n",
        "# filename_list=['test.txt', 'dev.txt', 'train.txt']\n",
        "# new_file_list = ['new_test.txt', 'new_dev.txt', 'new_train.txt']\n",
        "# file_dir='/content/drive/MyDrive/21-1_SoJong/20210407/mnli_input'\n",
        "# for i in range(len(filename_list)):\n",
        "#   dir = os.path.join(file_dir, filename_list[i])\n",
        "#   new_dir = os.path.join(file_dir, new_file_list[i])\n",
        "#   with open(dir) as f: \n",
        "#     path_list = f.read().splitlines()\n",
        "#   path_list = path_list[len(path_list)//2:]\n",
        "#   print(len(path_list))\n",
        "#   with open(new_dir, 'a') as f:\n",
        "#     f.write('\\n'.join(path_list)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28500\n",
            "30525\n",
            "243525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko1FWaq6qu-Z"
      },
      "source": [
        "mnli 결과 -1 핸들링 하려고 만든 데이터프레임들\n",
        "\n",
        "df_test, df_train, df_dev --> except_dump_xxx.csv 로 저장됨\n",
        "\n",
        "---\n",
        "mnli 결과 -1 핸들링 하기 위한 덤프 인덱스 목록\n",
        "\n",
        "train_dump, test_dump, dev_dump \n",
        "\n",
        "---\n",
        "원래 mnli 결과 그대로 가지는 데이터 프레임들\n",
        "\n",
        "train_result, test_result, dev_result -->xxx_result.csv 로 저장됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWGWZlwuVN4I"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 0 : contradiction 1 : neutral 2 : entailment\n",
        "df_test = pd.DataFrame(contrad_file['test'][4], columns = ['result'])\n",
        "df_test['result'] = df_test['result'].apply(int)\n",
        "df_train = pd.DataFrame(contrad_file['train'][4], columns = ['result'])\n",
        "df_train['result'] = df_train['result'].apply(int)\n",
        "df_dev = pd.DataFrame(contrad_file['dev'][4], columns = ['result'])\n",
        "df_dev['result'] = df_dev['result'].apply(int)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm7YCieRrG7M"
      },
      "source": [
        "dump 인덱스 목록 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWBZhJ7dVwiy"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open ('/content/drive/MyDrive/21-1_SoJong/20210407/mnli_input/dump_train_idx.txt','rb') as f:\n",
        "    train_dump = pickle.load(f)\n",
        "with open ('/content/drive/MyDrive/21-1_SoJong/20210407/mnli_input/dump_test_idx.txt','rb') as f:\n",
        "    test_dump = pickle.load(f)\n",
        "with open ('/content/drive/MyDrive/21-1_SoJong/20210407/mnli_input/dump_dev_idx.txt','rb') as f:\n",
        "    dev_dump = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e-d6ym_X_-q"
      },
      "source": [
        "for i in train_dump:\n",
        "    df_train['result'][i]=-1\n",
        "for i in test_dump:\n",
        "    df_test['result'][i]=-1\n",
        "for i in dev_dump:\n",
        "    df_dev['result'][i]=-1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4lSuM3VhU1L",
        "outputId": "52249a44-01f2-45f0-8691-2d1ab067347e"
      },
      "source": [
        "df_dev['result'][28]  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv-dQvqbVnSp"
      },
      "source": [
        "input_dir = '/content/drive/MyDrive/21-1_SoJong/20210407/mnli_result/'\n",
        "# train\n",
        "train_result = pd.DataFrame(contrad_file['train'][4], columns = ['result'])\n",
        "train_result['result'] = train_result['result'].apply(int)\n",
        "train_result.to_csv(input_dir + 'train_result.csv', index = False)\n",
        "df_train.to_csv(input_dir + 'except_dump_train_result.csv', index=False)\n",
        "\n",
        "# test\n",
        "test_result = pd.DataFrame(contrad_file['test'][4], columns = ['result'])\n",
        "test_result['result'] = test_result['result'].apply(int)\n",
        "test_result.to_csv(input_dir + 'test_result.csv', index = False)\n",
        "df_test.to_csv(input_dir + 'except_dump_test_result.csv', index=False)\n",
        "\n",
        "# dev\n",
        "dev_result = pd.DataFrame(contrad_file['dev'][4], columns = ['result'])\n",
        "dev_result['result'] = dev_result['result'].apply(int)\n",
        "dev_result.to_csv(input_dir + 'dev_result.csv', index = False)\n",
        "df_dev.to_csv(input_dir + 'except_dump_dev_result.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2tCaG4P9O5F"
      },
      "source": [
        "#### 수습 : 길이 반으로 자르기\n",
        "# dir = '/content/drive/MyDrive/21-1_SoJong/20210407/mnli_result'\n",
        "# train_result = pd.read_csv(os.path.join(dir, 'train_result.csv'))\n",
        "# train_result = train_result.iloc[len(train_result) // 2:]\n",
        "# df_train = pd.read_csv(os.path.join(dir, 'except_dump_train_result.csv'))\n",
        "# df_train = df_train.iloc[len(df_train) // 2:]\n",
        "# train_result.to_csv(os.path.join(dir,'train_result.csv'), index = False)\n",
        "# df_train.to_csv(os.path.join(dir,'except_dump_train_result.csv'), index=False)\n",
        "\n",
        "# dev_result = pd.read_csv(os.path.join(dir, 'dev_result.csv'))\n",
        "# dev_result = dev_result.iloc[len(dev_result) // 2:]\n",
        "# df_dev = pd.read_csv(os.path.join(dir, 'except_dump_dev_result.csv'))\n",
        "# df_dev = df_dev.iloc[len(df_dev) // 2:]\n",
        "# dev_result.to_csv(os.path.join(dir,'dev_result.csv'), index = False)\n",
        "# df_dev.to_csv(os.path.join(dir,'except_dump_dev_result.csv'), index=False)\n",
        "\n",
        "# test_result = pd.read_csv(os.path.join(dir, 'test_result.csv'))\n",
        "# test_result = test_result.iloc[len(test_result) // 2:]\n",
        "# df_test = pd.read_csv(os.path.join(dir, 'except_dump_test_result.csv'))\n",
        "# df_test = df_test.iloc[len(df_test) // 2:]\n",
        "# test_result.to_csv(os.path.join(dir,'test_result.csv'), index = False)\n",
        "# df_test.to_csv(os.path.join(dir,'except_dump_test_result.csv'), index=False)\n",
        "\n",
        "# # check\n",
        "# train_result = pd.read_csv(os.path.join(dir, 'train_result.csv'))\n",
        "# df_train = pd.read_csv(os.path.join(dir, 'except_dump_train_result.csv'))\n",
        "# dev_result = pd.read_csv(os.path.join(dir, 'dev_result.csv'))\n",
        "# df_dev = pd.read_csv(os.path.join(dir, 'except_dump_dev_result.csv'))\n",
        "# test_result = pd.read_csv(os.path.join(dir, 'test_result.csv'))\n",
        "# df_test = pd.read_csv(os.path.join(dir, 'except_dump_test_result.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtX0gmTx2pGF",
        "outputId": "09632582-6279-49ae-c66c-7e542416cf47"
      },
      "source": [
        "print(len(train_result), len(df_train))\n",
        "print(len(test_result), len(df_test))\n",
        "print(len(dev_result), len(df_dev))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "243525 243525\n",
            "28500 28500\n",
            "30525 30525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6HWh-IPuh98"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}